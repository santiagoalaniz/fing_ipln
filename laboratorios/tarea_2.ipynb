{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4azOYi8KSoC"
      },
      "source": [
        "# Segundo Laboratorio, iPLN\n",
        "\n",
        "El objetivo de este laboratorio es realizar diferentes experimentos para representar y clasificar textos. Para esto se trabajará con un corpus para análisis de sentimiento, creado para la competencia [TASS 2020](http://www.sepln.org/workshops/tass/) (IberLEF - SEPLN).\n",
        "\n",
        "## Entrega\n",
        "Deberán entregar un archivo *.ipynb* con su solución, que incluya código, comentarios y respuestas a las preguntas que se incluyen al final de este notebook.\n",
        "\n",
        "El plazo de entrega de la tarea 2 cierra el **20 de junio a las 23:59 horas**.\n",
        "\n",
        "## Plataforma sugerida\n",
        "Sugerimos que utilicen la plataforma [Google colab](https://colab.research.google.com/), que permite trabajar colaborativamente con un *notebook* de python. Al finalizar pueden descargar ese *notebook* en un archivo .ipynb, incluyendo las salidas ya ejecutadas, con la opción ```File -> Download -> Download .ipynb```.\n",
        "\n",
        "## Aprobación del laboratorio\n",
        "Para aprobar el laboratorio se exige como mínimo:\n",
        "* Probar dos enfoques diferentes para la representación de tweets (uno basado en BoW y otro en word embeddings)\n",
        "* Probar al menos dos modelos de aprendizaje automático con cada representación\n",
        "* Comparar los resultados con los obtenidos por el modelo de pysentimiento.\n",
        "El preprocesamiento, las pruebas con otras formas de representación de los tweets, los experimentos con otros modelos de aprendizaje automático, incluyendo aprendizaje profundo, entre otros posibles experimentos, no son requisito para aprobar el laboratorio, aunque aportan a la nota final.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAumcYFLP0f8"
      },
      "source": [
        "# **Parte 1 - Carga y preprocesamiento del corpus**\n",
        "\n",
        "`La aplicación de una etapa de preprocesamiento similar a la implementada en la tarea 1 es opcional.`\n",
        "\n",
        "Es interesante hacer experimentos con y sin la etapa de preprocesamiento, de modo de comparar resultados (sobre el corpus de desarrollo, devel.csv) y definir si se incluye o no en la solución final."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BuxSqllxpJD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cbabe0f-93bb-48c0-fb6c-949e7b40832c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from IPython.utils.text import format_screen\n",
        "# Importa las librerías necesarias\n",
        "import pandas as pd\n",
        "from google.colab import drive, files\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Carga de los datasets\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "devel_data = pd.read_csv('/content/drive/MyDrive/devel.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/test.csv')\n",
        "stop_words_df = pd.read_csv('/content/drive/MyDrive/stop_words_esp_anasent.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDT2SMIrch3O"
      },
      "source": [
        "## **Descripcion del proprocesamiento.**\n",
        "\n",
        "La aplicación de estas técnicas facilitará el análisis posterior del corpus y mejorará la calidad de los resultados obtenidos. Las tecnicas en cuestion:\n",
        "\n",
        "- `remove_urls`: Remplaza las URLs en un string con el string \"\".\n",
        "- `remove_hashtags`: Remplaza los hashtags en un string con el string \"\".\n",
        "- `remove_mentions`: Remplaza las menciones a usuarios en un string con el string \"Usuario\". Notar que los tweets al ser respondidos a otro usuario siempre llevan el ':' despues de la mención, consideramos esto dentro de la regex.\n",
        "- `remove_numbers`: Remplaza los números en un string con el string \"NUM\".\n",
        "- `remove_abbreviations`: Remplaza las abreviaciones comunes del español en un string con el término original, utilizando un diccionario predefinido.\n",
        "- `remove_repeated_symbols`: Remueve caracteres repetidos más de dos veces en un string.\n",
        "- `remove_laughts`: Remplaza diferentes expresiones de \"risas\" con el string \"jaja\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2kOeMA_ly_b"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Funciones de preprocesamiento\n",
        "## Remplaza urls con URL\n",
        "def remove_urls(x=str): return re.sub('((https|http)?://|www\\.)\\S+', '', x)\n",
        "## Remplaza hashtagas con HASHTAG\n",
        "def remove_hashtags(x=str): return re.sub('#\\w+', '', x)\n",
        "## Remplaza @usuario con USER\n",
        "def remove_mentions(x=str): return re.sub('@\\w+(\\:?)', 'usuario', x)\n",
        "## Remplaza el remitente '@usuario:' con nada\n",
        "def remove_writer(x=str): return re.sub('@\\w+\\:', '', x)\n",
        "## Remplaza secuencia de digitos con NUM\n",
        "def remove_numbers(x=str): return re.sub(r'\\d{1,3}(?:\\.\\d{3})*', 'dos', x)\n",
        "## Remplaza caracteres repetidos mas de dos veces\n",
        "def remove_rep_symbols(x=str): return re.sub(r'(\\w)\\1{2,}', r'\\1', x)\n",
        "## Remplaza diferentes onomatopeya de 'risas'\n",
        "def remove_laughts(x=str): return re.sub(r'([j|a]{1}[aeioujs]{3,}|[h|a]{1}[aeiouhs]{3,})\\w*', 'jaja', x)\n",
        "## Remplaza abreviaciones tipicas del espanol.\n",
        "def remove_abbreviations(text: str) -> str:\n",
        "  ABBREVIATIONS = {\n",
        "    r\"\\b1ro\\b\": \"primero\", r\"\\b2do\\b\": \"segundo\", r\"\\b3ro\\b\": \"tercero\", r\"\\bbna\\b\": \"buena\",\n",
        "    r\"\\bbnas\\b\": \"buenas\", r\"\\bbno\\b\": \"bueno\", r\"\\bbnos\\b\": \"buenos\", r\"\\bcm\\b\": \"como\", r\"\\bd\\b\": \"de\",\n",
        "    r\"\\bdsd\\b\": \"desde\", r\"\\bdr\\b(\\.?)\": \"doctor\", r\"\\bdnd\\b\": \"donde\", r\"\\bgral\\b(\\.?)\": \"general\",\n",
        "    r\"\\bhdp\\b\": \"hijo de puta\", r\"\\bhla\\b\": \"hola\", r\"\\bls\\b\": \"los\", r\"\\bm\\b\": \"me\", r\"\\bomg\\b\": \"oh mi dios\",\n",
        "    r\"\\bp\\b\": \"puede\", r\"\\bpq\\b\": \"por qué\", r\"\\bq\\b\": \"que\", r\"\\bq tal\\b\": \"qué tal\", r\"\\bsts\\b\": \"estás\",\n",
        "    r\"\\btb\\b\": \"también\", r\"\\btmb\\b\": \"también\", r\"\\bt\\b\": \"te\", r\"\\btd\\b\": \"todo\", r\"\\btds\\b\": \"todos\",\n",
        "    r\"\\bwtf\\b\": \"que carajos\", r\"\\bx\\b\": \"por\", r\"\\bxfa\\b\": \"por favor\", r\"\\bxq\\b\": \"porque\"\n",
        "  }\n",
        "\n",
        "  for abbr, full_word in ABBREVIATIONS.items(): text = re.sub(abbr, full_word, text, flags=re.IGNORECASE)\n",
        "\n",
        "  return text\n",
        "\n",
        "\n",
        "# Arreglo de funciones definidas anteriormente que se aplican a los tweets\n",
        "PIPELINE = [\n",
        "  remove_urls,\n",
        "  remove_mentions,\n",
        "  remove_writer,\n",
        "  remove_hashtags,\n",
        "  remove_numbers,\n",
        "  remove_abbreviations,\n",
        "  remove_laughts,\n",
        "  remove_rep_symbols,\n",
        "  lambda x: x.lower()\n",
        "]\n",
        "\n",
        "def apply_preprocessing(tweet_list: list[str]) -> list[str]:\n",
        "  clean_copy = list(tweet_list)\n",
        "  for i, tweet in enumerate(clean_copy):\n",
        "      for regex in PIPELINE:\n",
        "          tweet = regex(tweet)\n",
        "\n",
        "      clean_copy[i] = tweet\n",
        "\n",
        "  return clean_copy;\n",
        "\n",
        "train_tweets = train_data.iloc[:, 1].to_list()\n",
        "P_train_tweets = apply_preprocessing(tweet_list= train_tweets)\n",
        "\n",
        "devel_tweets = devel_data.iloc[:, 1].to_list()\n",
        "P_devel_tweets = apply_preprocessing(tweet_list= devel_tweets)\n",
        "\n",
        "test_tweets = test_data.iloc[:, 1].to_list()\n",
        "P_test_tweets = apply_preprocessing(tweet_list= test_tweets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFvzTtYogZpG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c2207af-b60c-4a43-8fa5-7b5c41816434"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original train tweet: Me volvieron a dejar sola\n",
            "Preprocessed train tweet: me volvieron a dejar sola\n",
            "Original devel tweet: Cuenta atrás para la era Zapatero. En unos minutos, ex presidente\n",
            "Preprocessed devel tweet: cuenta atrás para la era zapatero. en unos minutos, ex presidente\n",
            "Original test tweet: Sólo quería hablar con gente en verda \n",
            "Preprocessed test tweet: sólo quería hablar con gente en verda \n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "# Add a random index for train_tweets\n",
        "random_index_train = random.randint(0, len(train_tweets) - 1)\n",
        "\n",
        "# Print the original and preprocessed tweet for train_tweets at the random index\n",
        "print(\"Original train tweet:\", train_tweets[random_index_train])\n",
        "print(\"Preprocessed train tweet:\", P_train_tweets[random_index_train])\n",
        "\n",
        "# Add a random index for devel_tweets\n",
        "random_index_devel = random.randint(0, len(devel_tweets) - 1)\n",
        "\n",
        "# Print the original and preprocessed tweet for devel_tweets at the random index\n",
        "print(\"Original devel tweet:\", devel_tweets[random_index_devel])\n",
        "print(\"Preprocessed devel tweet:\", P_devel_tweets[random_index_devel])\n",
        "\n",
        "# Add a random index for test_tweets\n",
        "random_index_test = random.randint(0, len(test_tweets) - 1)\n",
        "\n",
        "# Print the original and preprocessed tweet for test_tweets at the random index\n",
        "print(\"Original test tweet:\", test_tweets[random_index_test])\n",
        "print(\"Preprocessed test tweet:\", P_test_tweets[random_index_test])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZMlbsw2uFLm"
      },
      "source": [
        "# Parte 2 - Representación de los tweets\n",
        "\n",
        "**Word Embeddings**\n",
        "\n",
        "* A partir de los word embeddings, representar cada tweet como el vector promedio (mean vector) de los vectores de las palabras que lo componen.\n",
        "* A partir de los word embeddings, representar cada tweet como la concatenación de los vectores de las palabras que lo componen (llevando el vector total a un largo fijo).\n",
        "\n",
        "Se recomienda trabajar con alguna de las colecciones de word embeddings disponibles en https://github.com/dccuchile/spanish-word-embeddings. El repositorio incluye links a ejemplos y tutoriales.\n",
        "\n",
        "\n",
        "Se pide que prueben al menos una opción basada en BoW y una basada en word embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ng5er9O_-B4"
      },
      "outputs": [],
      "source": [
        "# Instalación de spacy para lematizar\n",
        "\n",
        "!pip install -U spacy\n",
        "!python -m spacy download es_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5dn0pfA35pX"
      },
      "outputs": [],
      "source": [
        "# Modelo de lenguaje\n",
        "import es_core_news_sm\n",
        "MODEL_NAME = 'es_core_news_sm'\n",
        "nlp = es_core_news_sm.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oi_7FQ27_9-W"
      },
      "source": [
        "## **Word Embeddings**\n",
        "\n",
        "El siguiente fragmento de código importa varias bibliotecas y carga los recursos necesarios para manipular y procesar textos en español.\n",
        "\n",
        "Aquí está lo que hace cada línea:\n",
        "\n",
        "1. **Importaciones**: Las primeras líneas importan varias bibliotecas necesarias para la ejecución del código.\n",
        "\n",
        "2. **Constantes**: Luego se definen varias constantes. `MODEL_NAME` es el nombre del modelo de lenguaje español de SpaCy a cargar, `VEC_LIMIT` es el número máximo de vectores a cargar desde el archivo de vectores de palabras, `VEC_N` es la dimensión de los vectores de palabras y `VEC_NAME` es el nombre del archivo que contiene los vectores de palabras.\n",
        "\n",
        "3. **Carga de modelos**: Finalmente, se cargan los modelos necesarios para la ejecución del código. `we` es el modelo de vectores de palabras, que se carga desde el archivo especificado por `VEC_NAME`, limitando el número de vectores a cargar a `VEC_LIMIT`. `nlp` es el modelo de lenguaje de SpaCy, que se carga especificando `MODEL_NAME`.\n",
        "\n",
        "En resumen, este código está preparando el entorno para procesar texto en español utilizando el modelo de lenguaje de SpaCy y los vectores de palabras de Gensim. Estos vectores de palabras representarán semánticamente el texto en el espacio vectorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4WGR9XBCKQh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28d9381e-69b3-4603-a606-b3137724f8a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data  wiki.es.vec  wiki.es.vec.1\n"
          ]
        }
      ],
      "source": [
        "# Descargar el archivo con los vectores\n",
        "## La coleccion de vectores de la wikipedia tiene (#dimensions=300, #vectors=985,667) (2.4GBytes)\n",
        "!wget -q https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.es.vec\n",
        "# Verificar que se descargo el archivo wiki.es.vec\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaRgb-nEEwEx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24ca82b0-e88e-41d0-b089-c96888bbabd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.2)\n",
            "2023-06-20 23:00:08.214915: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting es-core-news-sm==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.5.0/es_core_news_sm-3.5.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-sm==3.5.0) (3.5.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "# Intalacion de librerias necesarias.\n",
        "## Word Embedding\n",
        "!pip -q install numpy gensim scikit-learn\n",
        "## Tokenizador\n",
        "!pip install -U spacy\n",
        "!python -m spacy download es_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFyYXHmUWHr2"
      },
      "outputs": [],
      "source": [
        "# Importa las librerías necesarias para manipular wiki.es.vec\n",
        "import logging\n",
        "import pdb\n",
        "import numpy as np\n",
        "import spacy\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "from numpy.linalg import norm\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "# Constantes de la realidad.\n",
        "VEC_LIMIT = 500000\n",
        "VEC_N = 300\n",
        "VEC_NAME = 'wiki.es.vec'\n",
        "\n",
        "# Herramientas de PLN.\n",
        "## Vectores, nuestro modelo es wiki.es.vec y cargamos 100k vectores (de los +900k posibles).\n",
        "we = KeyedVectors.load_word2vec_format(VEC_NAME, limit= VEC_LIMIT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzARx0-CSXgn"
      },
      "source": [
        "### **Representacion de los tweets como el \"mean vector\".**\n",
        "\n",
        "Este código define una función llamada `tweet_mean_vector` que toma como entrada una cadena de texto (en este caso, un \"tweet\") y devuelve un vector que representa una sintesis del contenido semántico de ese tweet.\n",
        "\n",
        "Para lograr esto, el código sigue los siguientes pasos:\n",
        "\n",
        "1. **Tokenización**: Divide el tweet en palabras individuales con la ayuda de una biblioteca de procesamiento de lenguaje natural.\n",
        "\n",
        "2. **Filtrado de tokens**: Filtra los tokens y solo retiene aquellos que están presentes en el vocabulario del modelo de embedding de palabras (representado por `we`).\n",
        "\n",
        "3. **Extracción de vectores de palabras**: Para cada token que está en el vocabulario del modelo de word embedding, el código extrae su vector correspondiente. Estos vectores representan el significado de las palabras en un espacio de alta dimensión.\n",
        "\n",
        "4. **Cálculo del vector medio**: Finalmente, la función calcula y devuelve la media de todos los vectores de palabras extraídos. Esta operación produce un solo vector que sirve como una representación agregada de todos los tokens en el tweet.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLLM25OIS0VU"
      },
      "outputs": [],
      "source": [
        "def tweet_mean_vector(tweet):\n",
        "  # Tokenizamos el tweet, es importante pasar todo a minusculas por el vocabulario del word embedding\n",
        "  tokens = [x.text for x in nlp(tweet.lower())]\n",
        "  # Lista de tokens que se encuentran en el vocabulario de la coleccion de vectores\n",
        "  tokens_in_we_vocabulary = [token for token in tokens if token in we.key_to_index]\n",
        "  if not tokens_in_we_vocabulary:\n",
        "    return np.zeros(VEC_N)\n",
        "  # Vectores de los tokens presentes\n",
        "  tokens_vectors = [we[x] for x in tokens_in_we_vocabulary]\n",
        "  # Media de todos los vectores\n",
        "  return np.mean(tokens_vectors, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oktpv9u0WPYy"
      },
      "source": [
        "### **Representacion de los tweets como \"concatenacion de vectores\".**\n",
        "\n",
        "El siguiente código define una función llamada `tweet_vector_concat` que convierte un tweet en una secuencia de vectores de palabras. Esta función difiere de `tweet_vector` (discutido anteriormente) en que no calcula el vector medio de todos los vectores de palabras en un tweet. En cambio, retorna una secuencia de vectores, uno para cada token en el tweet, y limita la cantidad de vectores a un tamaño máximo predefinido (`MAX_TWEET_SIZE`).\n",
        "\n",
        "Estos son los pasos que sigue:\n",
        "\n",
        "1. **Tokenización**: Al igual que antes, divide el tweet en tokens y los convierte en minúsculas.\n",
        "\n",
        "2. **Filtrado de tokens**: Selecciona solo aquellos tokens que están en el vocabulario del modelo de embedding de palabras.\n",
        "\n",
        "3. **Extracción de vectores de palabras**: Extrae los vectores correspondientes a los tokens.\n",
        "\n",
        "4. **Relleno/Truncamiento de la secuencia de vectores**: Para garantizar que todas las secuencias de vectores tengan la misma longitud (es decir, `MAX_TWEET_SIZE`), la función realiza dos operaciones:\n",
        "   - Si un tweet tiene menos tokens que `MAX_TWEET_SIZE`, la función añade vectores nulos (vectores de ceros) hasta que la longitud de la secuencia de vectores sea igual a `MAX_TWEET_SIZE`.\n",
        "   - Si un tweet tiene más tokens que `MAX_TWEET_SIZE`, la función trunca la secuencia de vectores a `MAX_TWEET_SIZE`.\n",
        "\n",
        "5. **Concatenación**: La función finalmente concatena todos los vectores en la secuencia en un solo vector largo y lo retorna.\n",
        "\n",
        "- **Largo del tweet**: Dado que Twitter limita los tweets a 280 caracteres y el promedio de caracteres por palabra en español es de [aproximadamente 9](https://www.um.es/lacell/aelinco/contenido/pdf/51.pdf), podríamos estimar que el número promedio de palabras por tweet estaría en el rango de 30 a 35 palabras.\n",
        "\n",
        "En resumen, esta función transforma un tweet en un vector de longitud fija que representa la secuencia de palabras en el tweet.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4istfXaRqKb_"
      },
      "outputs": [],
      "source": [
        "# Twitter tiene un limite de 280 caracteres.\n",
        "# El promedio de caracteres por palabra en Espanol es 9\n",
        "MAX_TWEET_SIZE = 30\n",
        "\n",
        "def tweet_concat_vector(tweet):\n",
        "  # Tokenizamos el tweet, es importante pasar todo a minusculas por el vocabulario del word embedding\n",
        "  tokens = [x.text for x in nlp(tweet.lower())]\n",
        "  # Lista de tokens que se encuentran en el vocabulario de la coleccion de vectores\n",
        "  tokens_in_we_vocabulary = [token for token in tokens if token in we.key_to_index]\n",
        "  # Vectores de los tokens presentes\n",
        "  tokens_vectors = [we[x] for x in tokens_in_we_vocabulary]\n",
        "\n",
        "  if not tokens_in_we_vocabulary:\n",
        "    return np.zeros(VEC_N * MAX_TWEET_SIZE)\n",
        "\n",
        "  # Si el tweet tiene menos del maximo lo rellenamos con vectores nulos de largo VEC_N\n",
        "  while(len(tokens_vectors) < MAX_TWEET_SIZE):\n",
        "    tokens_vectors.append(np.zeros(VEC_N))\n",
        "\n",
        "  # Si el tweet tiene mas palabras que el maximo lo truncamos.\n",
        "  tokens_vectors = tokens_vectors[:MAX_TWEET_SIZE]\n",
        "\n",
        "  return np.concatenate(tokens_vectors)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDaJ7IH9qCwp"
      },
      "source": [
        "### **Resultados.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KNC8eMhftpp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fc3a3f4-aabd-40f0-e8d9-574baa82d5ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Tweet: Damasiado frío Alonso (PSOE) en sus críticas al #rajoynazo1 Parece un técnico, no un político.\n",
            "Random Tweet Mean Vector-First 25 Values: [-0.15085624 -0.10582194 -0.04401828 -0.04981082  0.08270106  0.09837361\n",
            "  0.04047659  0.01863089  0.05943478 -0.29973823  0.09407208  0.05679687\n",
            "  0.0808995  -0.01183365 -0.07609273  0.01983912  0.06629678  0.06087295\n",
            " -0.02978192 -0.00220739  0.1212978   0.0632063  -0.08818258 -0.2257691\n",
            "  0.08657965]\n",
            "Random Tweet Vector- Last 25 values - 8974:8899: []\n"
          ]
        }
      ],
      "source": [
        "# Select a random index\n",
        "i = np.random.randint(0, len(test_tweets))\n",
        "\n",
        "# Print the random tweet and its corresponding vector for the first 25 columns\n",
        "print(\"Random Tweet:\", test_tweets[i])\n",
        "print(\"Random Tweet Mean Vector-First 25 Values:\", tweet_mean_vector(test_tweets[i])[:25])\n",
        "print(\"Random Tweet Vector- Last 25 values - 8974:8899:\", tweet_concat_vector(test_tweets[i])[8974:8899])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxAcPhqgWHr2"
      },
      "source": [
        "\n",
        "# Parte 3 - Clasificación de los tweets\n",
        "\n",
        "Para la clasificación de los tweets es posible trabajar con dos enfoques diferentes:\n",
        "\n",
        "* Aprendizaje Automático basado en atributos: se pide probar al menos dos modelos diferentes, por ejemplo, Multi Layer Perceptron ([MLP](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier)) y Support Vector Machines ([SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC)), y usar al menos dos formas de representación de tweets (una basada en BoW y otra basada en word embeddings). Se publicó en eva un léxico de palabras positivas y negativas que puede ser utilizado para generar atributos.\n",
        "\n",
        "* Aprendizaje Profundo: se recomienda experimentar con alguna red recurrente como LSTM. En este caso deben representar los tweets an base a word embeddings.\n",
        "\n",
        "Deberán usar el corpus de desarrollo (devel.csv) para comparar resultados de diferentes experimentos, variando los valores de los hiperparámetros, la forma de representación de los tweets, el preprocesamiento, los modelos de AA, etc.\n",
        "\n",
        "Tanto para la evaluación sobre desarrollo como para la evaluación final sobre test se usará la medida [Macro-F1](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) (promedio de la medida F1 de cada clase)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIySU4yzPIea"
      },
      "source": [
        "## SVM + Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wf2o1HYtu7XJ"
      },
      "source": [
        "### Librerias y Extructuras auxiliares\n",
        "\n",
        "Este código realiza el preprocesamiento de datos para un experimento de aprendizaje automático en análisis de texto. Mapea las etiquetas de sentimientos a números, y genera dos tipos de vectores de palabras para cada tweet en los conjuntos de datos de entrenamiento y desarrollo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tpo8Ch-oPY8"
      },
      "outputs": [],
      "source": [
        "# Experimentos con Aprendizaje Automático y word embeddings\n",
        "from sklearn import svm\n",
        "import numpy as np\n",
        "\n",
        "# Mapeamos las etiquetas a enteros\n",
        "P = {\n",
        "    'N': 0,\n",
        "    'P': 1,\n",
        "    'NONE': 2,\n",
        "}\n",
        "\n",
        "# etiquetas mapeadas para cada tweet de train y test\n",
        "train_s = np.array([P[x] for x in train_data.iloc[:, 2].to_list()])\n",
        "devel_s = np.array([P[x] for x in devel_data.iloc[:, 2].to_list()])\n",
        "\n",
        "# mean y concat vectors de train y train (preprocesado)\n",
        "train_tweets_vectors = [[tweet_mean_vector(x), tweet_concat_vector(x)] for x in train_tweets]\n",
        "P_train_tweets_vectors = [[tweet_mean_vector(x), tweet_concat_vector(x)] for x in P_train_tweets]\n",
        "\n",
        "# mean y concat vectors de devel y devel (preprocesado)\n",
        "devel_tweets_vectors = [[tweet_mean_vector(x), tweet_concat_vector(x)] for x in devel_tweets]\n",
        "P_devel_tweets_vectors = [[tweet_mean_vector(x), tweet_concat_vector(x)] for x in P_devel_tweets]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbsXNjo3u_ga"
      },
      "source": [
        "### Diferentes construcciones del modelo.\n",
        "\n",
        "Este código entrena seis clasificadores SVM utilizando diferentes representaciones de los datos y diferentes metaparámetros:\n",
        "\n",
        "1. Un SVM estándar con `tol=0.01` entrenado con vectores promedio de tweets sin preprocesar.\n",
        "2. Un SVM estándar con `tol=0.01` entrenado con vectores promedio de tweets preprocesados.\n",
        "3. Un SVM estándar con `tol=0.3` y `cache_size=4096` entrenado con vectores concatenados de tweets sin preprocesar.\n",
        "4. Un SVM estándar con `tol=0.3` y `cache_size=4096` entrenado con vectores concatenados de tweets preprocesados.\n",
        "5. Un SVM con `class_weight='balanced'`, `tol=0.1`, `cache_size=1024`, y `decision_function_shape='ovo'` (configuración 'G01') entrenado con vectores promedio de tweets sin preprocesar.\n",
        "6. Un SVM con la misma configuración 'G01' pero `tol=0.001` y `cache_size=600` entrenado con vectores promedio de tweets preprocesados.\n",
        "\n",
        "Los metaparámetros son:\n",
        "\n",
        "- `tol`: Criterio de tolerancia para la detención del algoritmo. Un valor más pequeño puede llevar a una solución más precisa pero el algoritmo tardará más tiempo.\n",
        "- `cache_size`: Controla el tamaño de la caché de kernel en los algoritmos SVM. Un valor mayor puede acelerar el entrenamiento en máquinas con suficiente memoria.\n",
        "- `class_weight`: Maneja clases desequilibradas. Cuando se establece como 'balanced', el algoritmo ajusta los pesos de las clases de forma inversamente proporcional a las frecuencias de las clases.\n",
        "- `decision_function_shape`: Determina la forma de la función de decisión en problemas de clasificación multiclase. 'ovo' significa \"one-vs-one\", una estrategia donde se crea un clasificador para cada par de clases.\n",
        "\n",
        "Cada uno de estos clasificadores se entrena con el mismo corpus y conjunto de etiquetas (`train, train_s`), variando diferentes representaciones de los tweets (mean vector, concat vector). Los diferentes metaparámetros y representaciones resultan en diferentes rendimientos dependiendo del problema y los datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eRPHfu-0em6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "591d6fed-85b9-48cc-a8e8-ae4bd91498fa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(tol=0.01)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(tol=0.01)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(tol=0.01)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# Mean Vectors: SVM -  Standar (train Sin Preprocesar)\n",
        "std_svm_clf = svm.SVC(tol=0.01)\n",
        "std_svm_clf.fit([vectors[0] for vectors in train_tweets_vectors], train_s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pz5oV3T70eQl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "626c516e-f027-45dd-f9c8-1a52a042f360"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(tol=0.01)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(tol=0.01)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(tol=0.01)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# Mean Vectors: SVM -  Standar (train Preprocesado)\n",
        "std_svm_clf_P = svm.SVC(tol=0.01)\n",
        "std_svm_clf_P.fit([vectors[0] for vectors in P_train_tweets_vectors], train_s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7b3Sc330eEn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "0bbe552a-418a-4ae6-d73b-15be0eeeedf2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(cache_size=4096, tol=0.3)"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(cache_size=4096, tol=0.3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(cache_size=4096, tol=0.3)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# Concat Vectors: SVM -  Standar (train Sin Preprocesar)\n",
        "std_concat_svm_clf = svm.SVC(tol=0.3, cache_size=4096)\n",
        "std_concat_svm_clf.fit([vectors[1] for vectors in train_tweets_vectors], train_s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbX9fcZD1KBw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "069149e9-b6e6-4846-b916-1a858e4ccc98"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(cache_size=4096, tol=0.3)"
            ],
            "text/html": [
              "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(cache_size=4096, tol=0.3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(cache_size=4096, tol=0.3)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# Concat Vectors: SVM -  Standar (Preprocesado)\n",
        "std_concat_svm_clf_P = svm.SVC(tol=0.3, cache_size=4096)\n",
        "std_concat_svm_clf_P.fit([vectors[1] for vectors in P_train_tweets_vectors], train_s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMR3pObt0d0q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "a5fc9493-b391-4ca7-dd4e-e9f2292e03ec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(cache_size=1024, class_weight='balanced', decision_function_shape='ovo')"
            ],
            "text/html": [
              "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(cache_size=1024, class_weight=&#x27;balanced&#x27;, decision_function_shape=&#x27;ovo&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(cache_size=1024, class_weight=&#x27;balanced&#x27;, decision_function_shape=&#x27;ovo&#x27;)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# Mean Vectors: SVM -  G01 Clasifier (train Sin Preprocesar)\n",
        "G01_svm_clf = svm.SVC(class_weight='balanced', tol=0.001, cache_size=1024, decision_function_shape='ovo')\n",
        "G01_svm_clf.fit([vectors[0] for vectors in train_tweets_vectors], train_s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-pZ4VjXpWr_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "1873aa80-d79a-4d3a-d61d-318c402d6519"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(cache_size=1024, class_weight='balanced', decision_function_shape='ovo')"
            ],
            "text/html": [
              "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(cache_size=1024, class_weight=&#x27;balanced&#x27;, decision_function_shape=&#x27;ovo&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(cache_size=1024, class_weight=&#x27;balanced&#x27;, decision_function_shape=&#x27;ovo&#x27;)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# Mean Vectors: SVM -  G01 Clasifier (train Preprocesado)\n",
        "G01_svm_clf_P = svm.SVC(class_weight='balanced', tol=0.001, cache_size=1024, decision_function_shape='ovo')\n",
        "G01_svm_clf_P.fit([vectors[0] for vectors in P_train_tweets_vectors], train_s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iinM2g61vFDo"
      },
      "source": [
        "### Resultados\n",
        "\n",
        "\n",
        "| Clasificador           | Tipo         | Macro F1 Score   |\n",
        "|------------------------|--------------|------------------|\n",
        "| std_svm_clf            | mean_vector  | 0.6036           |\n",
        "| std_svm_clf_P          | mean_vector  | **0.6073**       |\n",
        "| std_concat_svm_clf     | concat_vector| 0.5631           |\n",
        "| std_concat_svm_clf_P   | concat_vector| 0.5501           |\n",
        "| G01_svm_clf            | mean_vector  | 0.5920          |\n",
        "| G01_svm_clf_P          | mean_vector  | 0.6047           |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Iw3NI8UOg7v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94aa7022-b245-4770-b70f-e0ea71a2923e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clasificador: std_svm_clf\n",
            "Tipo: mean_vector\n",
            "Macro F1 Score: 0.6035951921293453\n",
            "\n",
            "Clasificador: std_svm_clf_P\n",
            "Tipo: mean_vector\n",
            "Macro F1 Score: 0.6072846279050766\n",
            "\n",
            "Clasificador: std_concat_svm_clf\n",
            "Tipo: concat_vector\n",
            "Macro F1 Score: 0.5631376073945945\n",
            "\n",
            "Clasificador: std_concat_svm_clf_P\n",
            "Tipo: concat_vector\n",
            "Macro F1 Score: 0.5501043123088898\n",
            "\n",
            "Clasificador: G01_svm_clf\n",
            "Tipo: mean_vector\n",
            "Macro F1 Score: 0.5974552392537711\n",
            "\n",
            "Clasificador: G01_svm_clf_P\n",
            "Tipo: mean_vector\n",
            "Macro F1 Score: 0.6046903913371217\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "svm_we_results = [\n",
        "    {\n",
        "        'clf': 'std_svm_clf',\n",
        "        'type': 'mean_vector',\n",
        "        'devel_prediction': std_svm_clf.predict([vectors[0] for vectors in devel_tweets_vectors]),\n",
        "    },\n",
        "    {\n",
        "        'clf': 'std_svm_clf_P',\n",
        "        'type': 'mean_vector',\n",
        "        'devel_prediction': std_svm_clf_P.predict([vectors[0] for vectors in P_devel_tweets_vectors]),\n",
        "    },\n",
        "    {\n",
        "        'clf': 'std_concat_svm_clf',\n",
        "        'type': 'concat_vector',\n",
        "        'devel_prediction': std_concat_svm_clf.predict([vectors[1] for vectors in devel_tweets_vectors]),\n",
        "    },\n",
        "    {\n",
        "        'clf': 'std_concat_svm_clf_P',\n",
        "        'type': 'concat_vector',\n",
        "        'devel_prediction': std_concat_svm_clf_P.predict([vectors[1] for vectors in P_devel_tweets_vectors]),\n",
        "    },\n",
        "    {\n",
        "        'clf': 'G01_svm_clf',\n",
        "        'type': 'mean_vector',\n",
        "        'devel_prediction': G01_svm_clf.predict([vectors[0] for vectors in devel_tweets_vectors]),\n",
        "    },\n",
        "    {\n",
        "        'clf': 'G01_svm_clf_P',\n",
        "        'type': 'mean_vector',\n",
        "        'devel_prediction': G01_svm_clf_P.predict([vectors[0] for vectors in P_devel_tweets_vectors]),\n",
        "    },\n",
        "]\n",
        "\n",
        "\n",
        "for result in svm_we_results:\n",
        "    print(f\"Clasificador: {result['clf']}\")\n",
        "    print(f\"Tipo: {result['type']}\")\n",
        "    devel_prediction = result['devel_prediction']\n",
        "\n",
        "    # devel_s son las etiquetas verdaderas\n",
        "    macro_f1 = f1_score(devel_s, devel_prediction, average='macro')\n",
        "    print(f\"Macro F1 Score: {macro_f1}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6VGFrEoWHr3"
      },
      "source": [
        "# Parte 4: Evaluación sobre test\n",
        "\n",
        "Deben probar los mejores modelos obtenidos en la parte anterior sobre el corpus de test.\n",
        "\n",
        "También deben comparar sus resultados con un modelo pre-entrenado para análisis de sentimientos de la biblioteca [pysentimiento](https://github.com/pysentimiento/pysentimiento) (deben aplicarlo sobre el corpus de test).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qu6GcEVIZkDI"
      },
      "outputs": [],
      "source": [
        "# tags de test\n",
        "test_s = np.array([P[x] for x in test_data.iloc[:, 2].to_list()])\n",
        "\n",
        "# WE + SVM\n",
        "## mean vectors del corpus test (preprocesado)\n",
        "P_test_mean_vectors = [tweet_mean_vector(x) for x in P_test_tweets]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU6YKdgIW3ch"
      },
      "source": [
        "## Nuestros mejores modelos y sus resultados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OzTMYHtsq5S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30248c5c-0f7e-4184-9256-53503b8d9d8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WE+SVM Macro F1 Score (Preproccessed Test Corpus): 0.6085469536471794\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# WE+SVM\n",
        "svm_predictions = std_svm_clf_P.predict(P_test_mean_vectors)\n",
        "macro_f1 = f1_score(test_s, svm_predictions, average='macro')\n",
        "print(f\"WE+SVM Macro F1 Score (Preproccessed Test Corpus): {macro_f1}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FklkcEOzWSLF"
      },
      "source": [
        "## Resultados de modelo externo: pysentimiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSTugJoM4AZ-"
      },
      "outputs": [],
      "source": [
        "!pip install pysentimiento\n",
        "!pip install transformers[torch]\n",
        "!pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LBEzvXlWWZ4"
      },
      "outputs": [],
      "source": [
        "from pysentimiento import create_analyzer\n",
        "analyzer = create_analyzer(task=\"sentiment\", lang=\"es\")\n",
        "\n",
        "P = {\n",
        "    'NEG': 0,\n",
        "    'POS': 1,\n",
        "    'NEU': 2,\n",
        "}\n",
        "\n",
        "predictions = [ P[analyzer.predict(x).output] for x in P_test_tweets ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hC1IjHmSI9-"
      },
      "outputs": [],
      "source": [
        "macro_f1 = f1_score(test_s, predictions, average='macro')\n",
        "print(f\"Pysentimiento F1 Score (Preproccessed Test Corpus): {macro_f1}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rn-TGtvvst3T"
      },
      "source": [
        "## Preguntas finales\n",
        "\n",
        "Responda las siguientes preguntas:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) ¿Qué modelos probaron para la representación de los tweets?\n",
        "- Word Embeddings: mean (300) y concat vectors (9k)"
      ],
      "metadata": {
        "id": "nE8dr9lYhCkN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) ¿Qué modelos de aprendizaje automático probaron?\n",
        "- svc() de sklearn\n"
      ],
      "metadata": {
        "id": "JM3eZBYQhLjY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 8) ¿Qué clase es la mejor clasificada por este enfoque? ¿Cuál es la peor? ¿Por qué piensan que sucede esto?"
      ],
      "metadata": {
        "id": "ifwTdWF4hkMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f1 = f1_score(y_true=test_s, y_pred=svm_predictions, labels=[0,1,2], average=None)\n",
        "print('NEG, POS, NEU')\n",
        "print(f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kO7zgQDJkQ6Q",
        "outputId": "bd15aab9-0a7b-459b-e7f6-80b9b6325c14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NEG, POS, NEU\n",
            "[0.6271722  0.68303914 0.51542952]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Según estos resultados:\n",
        "\n",
        "    La clase NEG (Negativa) tiene una puntuación F1 de 0.627.\n",
        "    La clase POS (Positiva) tiene una puntuación F1 de 0.683.\n",
        "    La clase NEU (Neutra) tiene una puntuación F1 de 0.515.\n",
        "\n",
        "Por lo tanto, según la puntuación F1, el modelo clasifica mejor los tweets positivos (POS), seguidos de los tweets negativos (NEG), y finalmente los tweets neutrales (NEU). Esto significa que el modelo es más preciso y tiene un mejor recall al clasificar tweets positivos en comparación con los tweets negativos y neutrales.\n",
        "\n",
        "Una razon de la mala performance de los neutros puede ser:\n",
        "\n",
        "Claridad del sentimiento: Los tweets positivos y negativos pueden tener sentimientos más claros y distinguibles que los neutrales. Los tweets neutrales pueden ser más sutiles y difíciles de clasificar. Esto toma otro valor cuando consideramos que los we, son sensibles a la semantica del tweet."
      ],
      "metadata": {
        "id": "1GJCV565yCyr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9) ¿Cómo son sus resultados en comparación con los de pysentimiento? ¿Por qué piensan que sucede esto?\n",
        "\n",
        "`pysentimiento` arroja un macro f-score de 0.696, superior a nuestros modelos construidos en la parte anterior. Alguno de los motivos pueden ser:\n",
        "\n",
        "- **Orientado al contexto del corpus**: El readme del repositorio lo deja bien claro pysentimiento es \"*A Transformer-based library for SocialNLP tasks.*\", el hecho que fuera construido para detectar el sentimiento de frases en redes sociales (SocialNLP) nos hace sospechar de un desempeño superior a nuestros modelos basados en librerias de uso general."
      ],
      "metadata": {
        "id": "UXSghPTfhpSp"
      }
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}